{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for pooled analysis\n",
    "\n",
    "### Content\n",
    "\n",
    "+ [1. Notebook description](#1.-Notebook-Description)\n",
    "+ [2. Pooled Data](#2.-pooled-data)\n",
    "+ [3. Individual Subjects](#3.-individual-subjects)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Notebook Description\n",
    "\n",
    "To run the pooled analysis we have to transform the data for each subject according to some model and then combine it.\n",
    "We load a model config file, transform the data according to the config object and concatenate all samples together in a single dataframe. We can do this, because the row index is unique over all sessions, trials, presentations *and* subjects.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Imports:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from digits.data import matimport, select, utils\n",
    "from digits.utils import getoutname, dotdict\n",
    "from digits.transform.dimreduction import SubsampleTransform, FFTransform\n",
    "\n",
    "import yaml\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the subject IDs and which models (transformation schemes to use):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subjects = [3130, 3131, 3132, 3134, 3135, 3136, 3138, 3146, 3147, 3149,\n",
    "            3154, 3156, 3157, 3158, 3159, 3161, 3162, 3233, 3237, 3239,\n",
    "            3240, 3241, 3242, 3243, 3245, 3248, 3250, 3251, 3252, 3253,\n",
    "            3255, 3260]\n",
    "\n",
    "configs = ['short_lda_1.yaml', 'short_lda_4.yaml', 'short_nofft20.yaml']\n",
    "dataroot = '../../../data/thomas/artcorr/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pooled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for config_file in configs:\n",
    "    with open('../../../jobs/configs/'+config_file, 'r') as f:\n",
    "        config = dotdict(yaml.load(f)['config'])\n",
    "        \n",
    "    for subject in [str(x) for x in subjects]:\n",
    "        \n",
    "        outfile = config_file+'.h5'\n",
    "        if path.exists(path.join(dataroot, 'transformed', outfile)):\n",
    "            print('skipping: '+outfile)\n",
    "            continue\n",
    "    \n",
    "        imp = matimport.Importer(dataroot=dataroot)\n",
    "        imp.open(subject+'.h5')\n",
    "\n",
    "        samples = imp.store.samples\n",
    "        targets = imp.store.targets\n",
    "        samples = select.fromtimerange(samples, config.t0, config.t1)\n",
    "        samples, targets = select.fromsessionblacklist(samples, targets, ['01'])\n",
    "        samples = select.fromchannelblacklist(samples, ['LHEOG', 'RHEOG', 'IOL'])\n",
    "        samples = SubsampleTransform(width=config.subsample_width, verbose=True).transform(samples)\n",
    "        if config.fft:\n",
    "            samples = FFTransform(verbose=True, bins=config.size, fmin=config.fmin, fmax=config.fmax,\n",
    "                                  power=config.power, rate=config.subsample_width/1000.).transform(samples)\n",
    "        if 'samples_all' in locals():\n",
    "            samples_all = samples_all.append(samples, verify_integrity=True)\n",
    "            targets_all = targets_all.append(targets, verify_integrity=True)\n",
    "        else:\n",
    "            samples_all = samples\n",
    "            targets_all = targets\n",
    "    \n",
    "    samples_all = utils.remove_duplicate_columns(samples_all, factor=2)\n",
    "    pool = matimport.Importer(dataroot=dataroot)\n",
    "    pool.ds = dotdict({\n",
    "        'samples': samples_all,\n",
    "        'targets': targets_all,\n",
    "    })\n",
    "    pool.save(outfile)\n",
    "    del samples_all, targets_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. individual subjects\n",
    "\n",
    "Do this again for each individual subject, without pooling, in case we need the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for config_file in configs:\n",
    "    with open('../../../jobs/configs/'+config_file, 'r') as f:\n",
    "        config = dotdict(yaml.load(f)['config'])\n",
    "        \n",
    "    for subject in [str(x) for x in subjects]:\n",
    "    \n",
    "        outfile = subject+'_'+config_file+'.h5'\n",
    "        if path.exists(path.join(dataroot, 'transformed', outfile)):\n",
    "            print('skipping: '+outfile)\n",
    "            continue\n",
    "        \n",
    "        imp = matimport.Importer(dataroot=dataroot)\n",
    "        imp.open(subject+'.h5')\n",
    "\n",
    "        samples = imp.store.samples\n",
    "        targets = imp.store.targets\n",
    "        samples = select.fromtimerange(samples, config.t0, config.t1)\n",
    "        samples, targets = select.fromsessionblacklist(samples, targets, ['01'])\n",
    "        samples = select.fromchannelblacklist(samples, ['LHEOG', 'RHEOG', 'IOL'])\n",
    "        samples = SubsampleTransform(width=config.subsample_width, verbose=True).transform(samples)\n",
    "        if config.fft:\n",
    "            samples = FFTransform(verbose=True, bins=config.size, fmin=config.fmin, fmax=config.fmax,\n",
    "                                  power=config.power, rate=config.subsample_width/1000.).transform(samples)\n",
    "\n",
    "    \n",
    "        samples = utils.remove_duplicate_columns(samples, factor=2)\n",
    "        pool = matimport.Importer(dataroot=dataroot)\n",
    "        pool.ds = dotdict({\n",
    "            'samples': samples,\n",
    "            'targets': targets,\n",
    "        })\n",
    "\n",
    "        pool.save(outfile)\n",
    "        print('saved: '+outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
